---
output:
  pdf_document: default
  html_document: default
---

# Intro to Data Science - Lab 11
##### Copyright 2022, Jeffrey Stanton and Jeffrey Saltz   Please do not post online.

## Week 11 - Text Mining

```{r}
# Enter your name here: Alireza Zarrinmehr
```

### Please include nice comments. <br>
### Instructions: 
Run the necessary code on your own instance of R-Studio.

### Attribution statement: (choose only one and delete the rest)

```{r}
# 1. I did this lab assignment by myself, with help from the book and the professor.

```

**Text mining** plays an important role in many industries because of
the prevalence of text in the interactions between customers and company
representatives. Even when the customer interaction is by speech, rather than by
chat or email, speech to text algorithms have gotten so good that transcriptions of
these spoken word interactions are often available. <br><br>To an increasing extent, a data
scientist needs to be able to wield tools that turn a body of text into actionable
insights.<br><br>
In this exercise, we work with a small number of social media posts on the topic of
climate change. Make sure to install and library the **tidiverse**, **quanteda**, **quanteda.textplots**, and **quanteda.textstats** packages
before starting the exercise.

```{r}
#install.packages('tidyverse')
#install.packages('quanteda')
#install.packages('quanteda.textplots')
#install.packages('quanteda.textstats')
library(tidyverse)
library(quanteda)
library(quanteda.textplots)
library(quanteda.textstats)
```

1. Read in the following data set with **read_csv()**:
https://intro-datascience.s3.us-east-2.amazonaws.com/ClimatePosts.csv <br>
The name of the file is ** ClimatePosts.csv **. Store the data in a data frame called **tweetDF**. Use **str(tweetDF)** to summarize the data. Add a comment describing
what you see. Make sure to explain what each of the three variables contains.

```{r}
tweetDF <- read_csv('https://intro-datascience.s3.us-east-2.amazonaws.com/ClimatePosts.csv')
str(tweetDF)
#ID (Document ID)
#Skeptic (is the user skeptic about the tweet or not?).
#Tweet: the words that user has tweeted.
```

2. Use the **corpus** commands to turn the text variable into a quanteda corpus. You
can use the IDs as the document titles with the following command:<br><br>
`tweetCorpus <- corpus(tweetDF$Tweet, docnames=tweetDF$ID)`

```{r}
#Turning the text variable into a quanteda corpus
tweetCorpus <- corpus(tweetDF$Tweet, docnames=tweetDF$ID)
```

3. Next, convert the corpus into a **document-feature matrix (DFM)**. Before you do
that you can use ** tokens ** to remove punctuation and stop words. Use this code and comment each line:<br><br>


```
toks <- tokens(tweetCorpus, remove_punct=TRUE)
toks_nostop <- tokens_select(toks, pattern = stopwords("en"), selection = "remove")
```


Here s a command that will create the DFM:<br><br>
`tweetDFM <- dfm(toks_nostop, tolower = TRUE )`

```{r}
#Removing the punctuations from the corpus.
toks <- tokens(tweetCorpus, remove_punct=TRUE)
#Removing the stopwords.
toks_nostop <- tokens_select(toks, pattern = stopwords("en"), selection = "remove")
```

4. Type **tweetDFM** in the code cell to find out the basic characteristics of the DFM (the
number of terms, the number of documents, and the sparsity of the matrix).
Write a comment describing what you observe.

```{r}
tweetDFM <- dfm(toks_nostop, tolower = TRUE )
tweetDFM
#the number of terms: 223 features
#the number of documents: 18 documents
#the sparsity of the matrix: 93.20% sparse
#We observe the times each word used in each document.
```

5. Create a **wordcloud** from the DFM using the following command. Write a
comment describing notable features of the wordcloud:<br><br>
`textplot_wordcloud(tweetDFM, min_count = 1)`

```{r}
textplot_wordcloud(tweetDFM, min_count = 1)
#The most used words are Climate, Global, Change,...
#These words are used at least one time.
```

6. Using **textstat_frequency()** from the **quanteda.textstats** package, show the 10
most frequent words, and how many times each was used/mentioned.

```{r}
textstat_frequency(tweetDFM, 10)
# We should address to frequency to see how many times each was used/mentioned.
```

7. Next, we will read in **dictionaries** of positive and negative words to see what we
can match up to the text in our DFM. Here s a line of code for reading in the list of
positive words:<br><br>


```
URL <- "https://intro-datascience.s3.us-east-2.amazonaws.com/positive-words.txt"
posWords <- scan(URL, character(0), sep = "\n")
posWords <- posWords[-1:-34]
```

 <br><br>
Create a similar line of code to read in the negative words, with the following
URL:<br><br> https://intro-datascience.s3.us-east-2.amazonaws.com/negative-words.txt  <br><br>
There should be 2006 positive words and 4783 negative words.

```{r}
URL <- "https://intro-datascience.s3.us-east-2.amazonaws.com/positive-words.txt"
posWords <- scan(URL, character(0), sep = "\n")
posWords <- posWords[-1:-34]

URL1 <- "https://intro-datascience.s3.us-east-2.amazonaws.com/negative-words.txt"
negWords <- scan(URL1, character(0), sep = "\n")
negWords <- negWords[-1:-34]
```

8. Explain what the following lines of code does and comment each line. Then add
similar code for the negative words.<br><br>


```
posDFM <- dfm_match(tweetDFM, posWords)
posFreq <- textstat_frequency(posDFM)
```


```{r}
#match the words from the data set to the positive words and show how many matches are there:
posDFM <- dfm_match(tweetDFM, posWords)
posFreq <- textstat_frequency(posDFM)
#match the words from the data set to the negative words and show how many matches are there:
negDFM <- dfm_match(tweetDFM, negWords)
negFreq <- textstat_frequency(negDFM)
```

9. Explore **posFreq** and **negFreq** using **str()** or **glimpse()**. Explain the fields in these data frames

```{r}
str(posFreq)
glimpse(posFreq)
str(negFreq)
glimpse(negFreq)
#frequency field shows:how many times the words are repeated.
#rank field shows:ranks regarding the frequency of the words.
#docfreq field shows: in how many documents are the words used.
```

10. Output the 10 most frequently occurring positive and negative words
including how often each occurred.

```{r}
#positive words
posFreq[1:10,]
#negative words
negFreq[1:10,]
```

